{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52babf2f",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "* [Define parameters](#parameters)\n",
    "* [Class definitions](#class)\n",
    "* [Function definitions](#function)\n",
    "  * [Optimize KNN neighbours](#optimizeknn)\n",
    "  * [KNN](#knn)\n",
    "  * [Calculate accuracy](#accuracy)\n",
    "  * [MLP](#mlp)\n",
    "  * [Plot](#plot)\n",
    "  * [Main](#main)\n",
    "* [Run](#run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5619d5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import faiss\n",
    "from collections import Counter\n",
    "\n",
    "from pycm import ConfusionMatrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch import FloatTensor, LongTensor\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d6d08d",
   "metadata": {},
   "source": [
    "## Define parameters <a class=\"anchor\" id=\"parameters\"></a>\n",
    "\n",
    "Change EC_LEVEL, FP_TYPE and MODEL_TYPE as desired.\n",
    "\n",
    "We suggest to train 4 models for ec123 to compare the different approaches:\n",
    "drfp+knn, drfp+mlp, rxnfp+knn and rxnfp+mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fee270",
   "metadata": {},
   "outputs": [],
   "source": [
    "EC_LEVEL   = 'ec123' # 'ec1' or 'ec12' or 'ec123'\n",
    "FP_TYPE    = 'drfp'  # 'drfp' or 'rxnfp'\n",
    "MODEL_TYPE = 'knn'   # 'knn' or 'mlp'\n",
    "INFO       = f'Training {MODEL_TYPE.upper()} model using {FP_TYPE.upper()} fingerprints to predict {EC_LEVEL.upper()}.'\n",
    "\n",
    "DATA_PATH  = 'experiments/data/'\n",
    "MODEL_PATH = 'models'\n",
    "trainset = f'{DATA_PATH}{FP_TYPE}-0-{EC_LEVEL}-train.csv' # file with training dataset\n",
    "validset = f'{DATA_PATH}{FP_TYPE}-0-{EC_LEVEL}-valid.csv' # file with validation dataset\n",
    "testset  = f'{DATA_PATH}{FP_TYPE}-0-{EC_LEVEL}-test.csv'  # file with test dataset\n",
    "output   = f'{MODEL_PATH}/rheadb-{EC_LEVEL}_{FP_TYPE}_{MODEL_TYPE}' # file prefix for output\n",
    "\n",
    "modeltype = MODEL_TYPE\n",
    "\n",
    "OPTIMIZE_KNN = False # Set this to True to run function 'optimizeNumNeighbors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da2f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(MODEL_PATH).mkdir(exist_ok=True)\n",
    "Path('figures').mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb6e1db",
   "metadata": {},
   "source": [
    "## Class definitions <a class=\"anchor\" id=\"class\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5554e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReactionDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, label: str = \"label\"):\n",
    "        self.size = len(df)\n",
    "        self.label = label\n",
    "        self.X = FloatTensor(\n",
    "            np.array([x.astype(np.float32) for x in df.fps], dtype=np.float32)\n",
    "        )\n",
    "        self.y = LongTensor(df[self.label].to_numpy(dtype=np.int32))\n",
    "        self.fps = df[\"fps\"]\n",
    "        self.rxn_smiles = df[\"rxn_smiles\"]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (\n",
    "            self.X[i],\n",
    "            self.y[i],\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.fc1 = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = self.fc1(x)\n",
    "        tanh = self.tanh(hidden)\n",
    "        output = self.fc2(tanh)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eef8f51",
   "metadata": {},
   "source": [
    "## Function definitions <a class=\"anchor\" id=\"function\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371a19ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        dev = \"cuda:0\"\n",
    "    else:\n",
    "        dev = \"cpu\"\n",
    "\n",
    "    return torch.device(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bd9b0f",
   "metadata": {},
   "source": [
    "## Optimize KNN neighbours <a class=\"anchor\" id=\"optimizeknn\"></a>\n",
    "\n",
    "Optional function to find the optimal number of neighbors for the k-nearest neighbors algorithm.\n",
    "\n",
    "If you run this by <a href=#parameters>setting the parameter <code>OPTIMIZE_KNN = True</code></a>, you will see that the optimal number of neighbors is 2, which is the default value of the function <code>get_nearest_neighbours_prediction</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649bfbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizeNumNeighbors(df_train, df_test):\n",
    "    print('Testing different numbers of neighbors for KNN algorithm.')\n",
    "    accuracies = []\n",
    "    X_train, y_train = np.array(df_train['fps'].to_list()), df_train['label']\n",
    "    X_test, y_test = np.array(df_test['fps'].to_list()), df_test['label']\n",
    "    for i in range(1,21):\n",
    "        y_pred = get_nearest_neighbours_prediction(X_train, y_train, X_test, i)\n",
    "        accuracy = getSklearnAccuracy(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    plt.scatter(range(1,21), accuracies)\n",
    "    plt.savefig(f'figures/knn_testing.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb50a2d3",
   "metadata": {},
   "source": [
    "### KNN <a class=\"anchor\" id=\"knn\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4e0e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_neighbours_prediction(\n",
    "    train_X: np.array, train_y: np.array, eval_X: np.array, n_neighbours: int = 2\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Use faiss to make a K-nearest neighbour prediction\n",
    "    \"\"\"\n",
    "    # Indexing\n",
    "    print(\"\\nKNN number of neighbours is\", n_neighbours)\n",
    "    index = faiss.IndexFlatL2(len(train_X[0]))\n",
    "    index.add(train_X.astype(np.float32))\n",
    "\n",
    "    # Querying\n",
    "    _, results = index.search(eval_X.astype(np.float32), n_neighbours)\n",
    "\n",
    "    # Scoring\n",
    "    y_pred = get_pred(train_y, results)\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def get_pred(y: list, results: list) -> list:\n",
    "    \"\"\"\n",
    "    Get most common label from nearest neighbour list.\n",
    "    \"\"\"\n",
    "    y_pred = []\n",
    "    for i, r in enumerate(results):\n",
    "        y_pred.append(Counter(y[r]).most_common(1)[0][0])\n",
    "        \n",
    "    return y_pred\n",
    "\n",
    "def trainKNN(X_train, y_train):\n",
    "    \"\"\"\n",
    "    The scikit-learn version of knn. Deprecated since it does not give as good results as faiss.\n",
    "    \"\"\"\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn_model.fit(X_train, y_train)\n",
    "\n",
    "    return knn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ab36d2",
   "metadata": {},
   "source": [
    "## Calculate accuracy <a class=\"anchor\" id=\"accuracy\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ea02ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for KNN.\n",
    "def getSklearnAccuracy(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    The default function to calculate accuracy with scikit learn.\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\n=> Accuracy according to scikit learn: {accuracy:.2f}\")\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Used for MLP.\n",
    "def get_accuracy(model, device, data_loader):\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "\n",
    "    y = []\n",
    "    y_pred = []\n",
    "\n",
    "    for data, labels in data_loader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        target = model(data)\n",
    "        pred = target.max(1, keepdim=True)[1] # Get the index of the max logit.\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        total += int(labels.shape[0])\n",
    "\n",
    "        y.extend(torch.flatten(labels.view_as(pred)).tolist())\n",
    "        y_pred.extend(torch.flatten(pred).tolist())\n",
    "\n",
    "    return correct / total, y, y_pred\n",
    "\n",
    "def confusionMatrixCalculate(y, y_pred):\n",
    "    cm = ConfusionMatrix(actual_vector=y, predict_vector=y_pred)\n",
    "    print('\\n=> Accuracy according to confusion matrix:')\n",
    "    #print(cm.overall_stat)\n",
    "    print(f\"   Accuracy: {cm.overall_stat['Overall ACC']:.2f}\")\n",
    "    print(f\"   MCC:      {cm.overall_stat['Overall MCC']:.2f}\")\n",
    "    print(f\"   CEN:      {cm.overall_stat['Overall CEN']:.2f}\")\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96730d29",
   "metadata": {},
   "source": [
    "## MLP <a class=\"anchor\" id=\"mlp\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25eeee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, valid_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    for _, (data, labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        # Set gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        target = model(data)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(target, labels)\n",
    "\n",
    "        # Calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate the training loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    valid_loss = 0.0\n",
    "    model.eval()\n",
    "    for data, labels in valid_loader:\n",
    "        if torch.cuda.is_available():\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        target = model(data)\n",
    "        loss = criterion(target, labels)\n",
    "        valid_loss += loss.item()\n",
    "\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    valid_loss = valid_loss / len(valid_loader)\n",
    "\n",
    "    train_acc, _, _ = get_accuracy(model, device, train_loader)\n",
    "    valid_acc, _, _ = get_accuracy(model, device, valid_loader)\n",
    "    print(f\"Epoch {epoch}: Training loss:     {train_loss}, Validation loss:     {valid_loss}.\")\n",
    "    print(f\"Epoch {epoch}: Training accuracy: {train_acc}, Validation accuracy: {valid_acc}.\")\n",
    "\n",
    "    return (train_loss, valid_loss, train_acc, valid_acc)\n",
    "\n",
    "def train_test_model(\n",
    "    data_set_train,\n",
    "    data_set_valid,\n",
    "    data_set_test,\n",
    "    label_encoder,\n",
    "    device,\n",
    "    input_dim: int = 10,\n",
    "    hidden_dim: int = 1664,\n",
    "    output_dim: int = 2,\n",
    "    epochs: int = 10,\n",
    "    patience: int = 5,\n",
    ") -> ConfusionMatrix:\n",
    "    \n",
    "    model = MLPClassifier(input_dim, hidden_dim, output_dim)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "    model.to(device)\n",
    "\n",
    "    training_matrics = {\n",
    "        \"train_loss\": [],\n",
    "        \"valid_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"valid_acc\": [],\n",
    "    }\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, valid_loss, train_acc, valid_acc = train(\n",
    "            model,\n",
    "            device,\n",
    "            DataLoader(data_set_train, batch_size=64),\n",
    "            DataLoader(data_set_valid, batch_size=64),\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            epoch,\n",
    "        )\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Early stopping based on past mean.\n",
    "        if len(training_matrics[\"valid_loss\"]) >= patience:\n",
    "           mean_past = np.mean(np.array(training_matrics[\"valid_loss\"][-patience:]))\n",
    "           if valid_loss - mean_past > -0.001:\n",
    "               break\n",
    "\n",
    "        training_matrics[\"train_loss\"].append(train_loss)\n",
    "        training_matrics[\"valid_loss\"].append(valid_loss)\n",
    "        training_matrics[\"train_acc\"].append(train_acc)\n",
    "        training_matrics[\"valid_acc\"].append(valid_acc)\n",
    "\n",
    "    plot(training_matrics[\"train_loss\"], training_matrics[\"valid_loss\"])\n",
    "\n",
    "    test_acc, y, y_pred = get_accuracy(\n",
    "        model, device, DataLoader(data_set_test, batch_size=64)\n",
    "    )\n",
    "    y = label_encoder.inverse_transform(y)\n",
    "    y_pred = label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "    print(f\"\\n=> Accuracy of test: {test_acc:.2f}\")\n",
    "\n",
    "    cm = confusionMatrixCalculate(y, y_pred)\n",
    "\n",
    "    return model, cm, training_matrics, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd496d0",
   "metadata": {},
   "source": [
    "## Plot <a class=\"anchor\" id=\"plot\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba90064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(train_loss, valid_loss):\n",
    "    \"\"\"\n",
    "    Plotting the progress of training the MLP\n",
    "    \"\"\"\n",
    "\n",
    "    plt.plot(train_loss, label=\"train_loss\")\n",
    "    plt.plot(valid_loss, label=\"valid_loss\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.title(f'{FP_TYPE}-{EC_LEVEL}')\n",
    "    plt.ylim(bottom=0)\n",
    "    \n",
    "    plt.savefig(f'figures/{FP_TYPE}-{EC_LEVEL}.png', format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06163ef2",
   "metadata": {},
   "source": [
    "## Main <a class=\"anchor\" id=\"main\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a68a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(trainset, validset, testset, output, modeltype):\n",
    "\n",
    "    print(INFO)\n",
    "    device = get_device()\n",
    "\n",
    "    model_file = Path(f\"{output}.pt\")\n",
    "    cm_file = Path(f\"{output}.cm\")\n",
    "    training_metrics_file = Path(f\"{output}.metrics.pkl\")\n",
    "    label_encoder_file = Path(f\"{output}-le.pkl\")\n",
    "\n",
    "    df_train = pd.read_csv(trainset)\n",
    "    df_valid = pd.read_csv(validset)\n",
    "    df_test  = pd.read_csv(testset)\n",
    "\n",
    "    df_train[\"fps\"] = df_train.fps.apply(\n",
    "        #lambda x: np.array(list(map(int, x.split(\";\"))))\n",
    "        lambda x: np.array(list(map(float, x.split(\";\"))))\n",
    "    )\n",
    "    df_valid[\"fps\"] = df_valid.fps.apply(\n",
    "        #lambda x: np.array(list(map(int, x.split(\";\"))))\n",
    "        lambda x: np.array(list(map(float, x.split(\";\"))))\n",
    "    )\n",
    "    df_test[\"fps\"] = df_test.fps.apply(\n",
    "        #lambda x: np.array(list(map(int, x.split(\";\")))))\n",
    "        lambda x: np.array(list(map(float, x.split(\";\")))))\n",
    "\n",
    "    df_train.label = df_train.label.astype(str)\n",
    "    df_valid.label = df_valid.label.astype(str)\n",
    "    df_test.label  = df_test.label.astype(str)\n",
    "\n",
    "    # Encode categorical EC class labels\n",
    "    le = LabelEncoder()\n",
    "    le.fit(pd.concat([df_train.label, df_valid.label, df_test.label]))\n",
    "\n",
    "    with open(label_encoder_file, \"wb\") as f:\n",
    "        pickle.dump(le, f)\n",
    "        \n",
    "    df_train[\"label\"] = le.transform(df_train.label)\n",
    "    df_valid[\"label\"] = le.transform(df_valid.label)\n",
    "    df_test[\"label\"]  = le.transform(df_test.label)\n",
    "\n",
    "    data_set_train = ReactionDataset(df_train)\n",
    "    data_set_valid = ReactionDataset(df_valid)\n",
    "    data_set_test  = ReactionDataset(df_test)\n",
    "    \n",
    "    # This just shows the accuracy for different KNN neighbour values,\n",
    "    # it does not modify the default used in get_nearest_neighbours_prediction.\n",
    "    if OPTIMIZE_KNN:\n",
    "        optimizeNumNeighbors(df_train, df_test)\n",
    "\n",
    "    if modeltype=='knn':\n",
    "        X_train, y_train = np.array(df_train['fps'].to_list()), df_train['label']\n",
    "        X_test, y_test = np.array(df_test['fps'].to_list()), df_test['label']\n",
    "        y_pred = get_nearest_neighbours_prediction(X_train, y_train, X_test)\n",
    "        getSklearnAccuracy(y_test, y_pred)\n",
    "        confusionMatrixCalculate(y_test.tolist(), y_pred)\n",
    "        df_test['predicted EC'] = le.inverse_transform(y_pred)\n",
    "\n",
    "    elif modeltype=='mlp':\n",
    "        n_classes = len(le.classes_)\n",
    "        input_dim = len(df_train[\"fps\"].iloc[0])\n",
    "        \n",
    "        model, cm, training_matrics, y_pred = train_test_model(\n",
    "            data_set_train,\n",
    "            data_set_valid,\n",
    "            data_set_test,\n",
    "            le,\n",
    "            device,\n",
    "            input_dim=input_dim,\n",
    "            hidden_dim=1664,\n",
    "            output_dim=n_classes,\n",
    "            epochs=100,\n",
    "        )\n",
    "\n",
    "        with open(cm_file, \"wb\") as f:\n",
    "            pickle.dump(cm, f)\n",
    "\n",
    "        with open(training_metrics_file, \"wb\") as f:\n",
    "            pickle.dump(training_matrics, f)\n",
    "\n",
    "        torch.save(model.state_dict(), model_file)\n",
    "\n",
    "        df_test['predicted EC'] = y_pred\n",
    "\n",
    "    df_test.drop(axis=1, columns='fps', inplace=True)\n",
    "    \n",
    "    csv_file = testset.replace('data/', '').replace('.csv', '') + '_with_prediction_' + modeltype + '.csv'\n",
    "    df_test.to_csv(csv_file)\n",
    "    print('\\n=> Created CSV file', csv_file)\n",
    "    print('=> Created model files:', glob.glob(f\"{output}*\"))\n",
    "    print('=> Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5114d80d",
   "metadata": {},
   "source": [
    "## Run <a class=\"anchor\" id=\"run\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b015252d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main(trainset, validset, testset, output, modeltype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rheaec",
   "language": "python",
   "name": "rheaec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
