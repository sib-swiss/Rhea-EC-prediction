{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4c4f496",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "* [Prepare project](#prepare)\n",
    "* [Function definitions](#function)\n",
    "  * [Encode DRFP](#drfp)\n",
    "  * [Encode RXNFP](#rxnfp)\n",
    "  * [Split into Train / Test / Validation](#split)\n",
    "  * [Main](#main)\n",
    "* [Run](#run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569886c8",
   "metadata": {},
   "source": [
    "## Prepare project <a class=\"anchor\" id=\"prepare\"></a>\n",
    "\n",
    "### Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc6fa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to 'drfp' or 'rxnfp'.\n",
    "fpencoder = 'drfp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6572214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "if fpencoder == 'rxnfp':\n",
    "    from rxnfp.transformer_fingerprints import RXNBERTFingerprintGenerator, get_default_model_and_tokenizer, generate_fingerprints\n",
    "elif fpencoder == 'drfp':\n",
    "    from drfp import DrfpEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f7a77d",
   "metadata": {},
   "source": [
    "### Create directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0532b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing paths is not recommended (you'd have to adapt the other notebooks accordingly).\n",
    "output_path = 'experiments/data/'\n",
    "Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "Path('data').mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf476c3",
   "metadata": {},
   "source": [
    "### Download and prepare input data\n",
    "Download the input files from the Rhea FTP site https://ftp.expasy.org/databases/rhea/tsv/ (internet connection required) and merge them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bf2597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from contextlib import closing\n",
    "\n",
    "# Get the curated Rhea-EC mapping.\n",
    "with closing(urllib.request.urlopen('https://ftp.expasy.org/databases/rhea/tsv/rhea2ec.tsv')) as r:\n",
    "    header = r.readline().decode(\"utf-8\")\n",
    "    with open('data/rhea_ec_directions_expanded.tsv', 'w') as w:\n",
    "        w.write(header.strip()+'\\trhea_exp\\n')\n",
    "        for line in r:\n",
    "            for i in range(4):\n",
    "                w.write(line.decode(\"utf-8\").strip())\n",
    "                w.write('\\t'+str(int(line.decode(\"utf-8\").split('\\t')[0])+i)+'\\n')\n",
    "                \n",
    "# Get the Rhea reaction SMILES and merge them with the EC mapping.\n",
    "df_smiles = pd.read_csv('https://ftp.expasy.org/databases/rhea/tsv/rhea-reaction-smiles.tsv', sep='\\t', header=None, names=['rhea_id','rxn_smiles'])\n",
    "df_ec = pd.read_csv('data/rhea_ec_directions_expanded.tsv', sep='\\t')\n",
    "df = df_ec.merge(df_smiles, how='right', left_on='rhea_exp',right_on='rhea_id')\n",
    "df.drop(columns=['DIRECTION', 'RHEA_ID', 'rhea_exp'], inplace=True)\n",
    "df.rename({'rhea':'id', 'ID':'ec', 'rxnsmiles':'rxn'}, axis=1, inplace=True)\n",
    "df.to_csv('data/rheadb.csv.gz', compression='gzip', index=False)\n",
    "\n",
    "if Path('data/rheadb.csv.gz').exists():\n",
    "    Path('data/rhea_ec_directions_expanded.tsv').unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd0d8ea",
   "metadata": {},
   "source": [
    "## Function definitions <a class=\"anchor\" id=\"function\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baf44dd",
   "metadata": {},
   "source": [
    "### Encode DRFP <a class=\"anchor\" id=\"drfp\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1369ec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeDRFP(df):\n",
    "    \n",
    "    df[\"fps\"] = DrfpEncoder.encode(\n",
    "        df.rxn_smiles,\n",
    "        show_progress_bar=True,\n",
    "        root_central_atom=False,\n",
    "        radius=2,\n",
    "        include_hydrogens=True,\n",
    "        n_folded_length=10240,\n",
    "    )\n",
    "    output = 'data/reactions_with_fp_encoded_drfp.tsv'\n",
    "    df.to_csv(output, sep='\\t')\n",
    "    print('=> Created file', output)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9109b5",
   "metadata": {},
   "source": [
    "### Encode RXNFP <a class=\"anchor\" id=\"rxnfp\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358feeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeRXNFP(df):\n",
    "    \n",
    "    model, tokenizer = get_default_model_and_tokenizer()\n",
    "\n",
    "    rxnfp_generator = RXNBERTFingerprintGenerator(model, tokenizer)\n",
    "    df[\"fps_unnormalized\"] = df.progress_apply(rxnfp_for_entry, axis=1, args=[rxnfp_generator,])\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    df[\"fps_unnormalized\"] = [str(i).replace('[', '').replace(']', '') for i in df[\"fps_unnormalized\"]]\n",
    "    array_fps = np.array([fp.split(', ') for fp in df[\"fps_unnormalized\"].to_list()])\n",
    "    array_fps = array_fps.astype(float)\n",
    "    scaler.fit(array_fps)\n",
    "    fit_array = scaler.transform(array_fps)\n",
    "    df[\"fps\"] = pd.Series(data=fit_array.tolist())\n",
    "    df.dropna(subset=['fps_unnormalized'], inplace=True)\n",
    "    \n",
    "    output = 'data/reactions_with_fp_encoded_rxnfp.tsv'\n",
    "    df.to_csv(output, sep='\\t')\n",
    "    print('=> Created file', output)\n",
    "    return df\n",
    "\n",
    "def rxnfp_for_entry(row, rxnfp_generator):\n",
    "    try:\n",
    "        fp = rxnfp_generator.convert(row.rxn_smiles)\n",
    "        return fp #','.join([str(i) for i in fp])\n",
    "    except Exception as e:\n",
    "        print('Exception FP:', e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512fed12",
   "metadata": {},
   "source": [
    "### Split into Train / Test / Validation <a class=\"anchor\" id=\"split\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7109c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitIntoTrainTestValidation(df):\n",
    "    for ec in [\"ec1\", \"ec12\", \"ec123\"]:\n",
    "        X = df.rxn_smiles.to_numpy()\n",
    "        y = df[ec].to_numpy()\n",
    "        fps = df.fps.to_numpy()\n",
    "        groups = df.ec_1.to_numpy()\n",
    "\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "        sss_valid = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "\n",
    "        for i, (train_index, test_valid_index) in enumerate(sss.split(X, groups)):\n",
    "            for _, (test_index, valid_index) in enumerate(\n",
    "                sss_valid.split(\n",
    "                    X[test_valid_index],\n",
    "                    groups[test_valid_index],\n",
    "                )\n",
    "            ):\n",
    "                X_train = X[train_index]\n",
    "                y_train = y[train_index]\n",
    "                fps_train = fps[train_index]\n",
    "\n",
    "                X_valid = X[valid_index]\n",
    "                y_valid = y[valid_index]\n",
    "                fps_valid = fps[valid_index]\n",
    "\n",
    "                X_test = X[test_index]\n",
    "                y_test = y[test_index]\n",
    "                fps_test = fps[test_index]\n",
    "\n",
    "                df_train = pd.DataFrame(\n",
    "                    {\n",
    "                        \"rxn_smiles\": X_train,\n",
    "                        \"label\": y_train,\n",
    "                        \"fps\": [\";\".join(map(str, fp)) for fp in fps_train],\n",
    "                    }\n",
    "                )\n",
    "                df_valid = pd.DataFrame(\n",
    "                    {\n",
    "                        \"rxn_smiles\": X_valid,\n",
    "                        \"label\": y_valid,\n",
    "                        \"fps\": [\";\".join(map(str, fp)) for fp in fps_valid],\n",
    "                    }\n",
    "                )\n",
    "                df_test = pd.DataFrame(\n",
    "                    {\n",
    "                        \"rxn_smiles\": X_test,\n",
    "                        \"label\": y_test,\n",
    "                        \"fps\": [\";\".join(map(str, fp)) for fp in fps_test],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                df_train.to_csv(f\"{output_path}{fpencoder}-{i}-{ec}-train.csv\", index=False)\n",
    "                df_valid.to_csv(f\"{output_path}{fpencoder}-{i}-{ec}-valid.csv\", index=False)\n",
    "                df_test.to_csv(f\"{output_path}{fpencoder}-{i}-{ec}-test.csv\", index=False)\n",
    "                \n",
    "    print('=> Created train, test and validation files in', output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba59c09",
   "metadata": {},
   "source": [
    "### Main <a class=\"anchor\" id=\"main\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8766d2a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Read the Rhea input data.\n",
    "    df = pd.read_csv('data/rheadb.csv.gz')\n",
    "    # Exclude reactions without EC.\n",
    "    df = df[df.ec.notna()]\n",
    "    df.drop_duplicates(subset=['MASTER_ID'], inplace=True)\n",
    "\n",
    "    df[[\"ec_1\", \"ec_2\", \"ec_3\", \"ec_4\"]] = df.ec.str.split(\".\", expand=True)\n",
    "    df[\"ec1\"]   = df.ec_1.astype(str)\n",
    "    df[\"ec12\"]  = df.ec_1.astype(str) + \".\" + df.ec_2.astype(str)\n",
    "    df[\"ec123\"] = df.ec_1.astype(str) + \".\" + df.ec_2.astype(str) + \".\" + df.ec_3.astype(str)\n",
    "    \n",
    "    # Remove transport reactions (EC class 7).\n",
    "    df = df[df.ec1 != \"7\"]\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    print('-> Creating', fpencoder, 'fingerprints for', len(df), 'unique Rhea reactions with curated EC (excluding EC7)')\n",
    "    if fpencoder == \"drfp\":\n",
    "        df = encodeDRFP(df)\n",
    "    elif fpencoder == \"rxnfp\":\n",
    "        df = encodeRXNFP(df)\n",
    "    else:\n",
    "        print('Error: Fingerprint encoder type must be drfp or rxnfp')\n",
    "    \n",
    "    print('-> Creating train, test, validation subsets for the training phase')\n",
    "    splitIntoTrainTestValidation(df)\n",
    "    \n",
    "    print('=> Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99882cf0",
   "metadata": {},
   "source": [
    "## Run  <a class=\"anchor\" id=\"run\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9115f723",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rheaec",
   "language": "python",
   "name": "rheaec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
